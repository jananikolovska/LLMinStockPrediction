{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a58cbc9-8f38-4d88-8536-09f558a37fd2",
   "metadata": {},
   "source": [
    "# AI for stock market prediction: Using LLMs for TimeSeries Predictions\n",
    "\n",
    "Project by: Jana Nikolovska <br>\n",
    "Supervised by: Giacomo Frisoni, MSc <br><!--  -->\n",
    "Prof. Gianluca Moro, PhD <br>\n",
    "\n",
    "ALMA MATER STUDITORIUM - University of Bologna\n",
    "May 2025\n",
    "\n",
    "---\n",
    "**Summary:** <br>\n",
    "In this project, I explore the use of Large Language Models (LLMs) for time series forecasting, focusing on the task of stock market prediction. The work was proposed and mentored by Prof. Gianluca Moro and Giacomo Frisoni at the University of Bologna.\n",
    "\n",
    "As a starting point, I used a provided notebook as starting point . The notebook introduces the dataset (historical S&P 500 data via [`yfinance`](https://pypi.org/project/yfinance/)), a baseline linear regression model for comparison, and the *Trading Protocol* — a framework to evaluate forecasting performance by simulating trading strategies.\n",
    "\n",
    "For the LLM-based forecasting approach, I followed the methodology described in [this paper](https://arxiv.org/pdf/2310.07820) and its [official implementation](https://github.com/ngruver/llmtime/tree/main). The code from the paper has been adapted and extended with additional functionality tailored to the specific requirements of my experiments.\n",
    "\n",
    "_Goal_: <br>\n",
    "The goal of this project was to create a similar problem to those of the referenced paper, particularly in terms of train and test sizes. The problem was defined such that with 150 days of value history, the goal was to predict the subsequent 29 days (for both open and closed time series), following an autoregressive approach without using ground truth. \n",
    "* Modifications were made to the Linear Regression model from the baseline notebook. While it still utilizes a lagged dataset for predictions, a simulation of autoregressiveness was incorporated to make it more comparable to autoregressive models. <br>\n",
    "The dataset was split into multiple 150-29 train-test sections, and models were trained and evaluated independently on each split. <br>\n",
    "\n",
    "_Evaluation_: <br>\n",
    "For evaluation, RMSE and MAPE were used to assess the models' predictive accuracy, while a trading protocol was employed to simulate trading and profit. To enhance the results, I averaged the outcomes across the different splits to achieve a more reliable measure of model performance. Various visualizations were included throughout the notebook to enhance the understanding of the results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "378acebe-2288-4c08-a940-96ac7317348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "from IPython.utils import io\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats\n",
    "import openai\n",
    "from data.serialize import SerializerSettings\n",
    "from models.utils import grid_iter\n",
    "from models.darts import get_arima_predictions_data\n",
    "from models.llmtime import get_llmtime_predictions_data\n",
    "from data.small_context import get_datasets\n",
    "from models.validation_likelihood_tuning import get_autotuned_predictions_data\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, mean_squared_log_error, median_absolute_error\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "from src.visualizations import random_8_digit_number, plot_autoregressive_ml_model_results, gain_over_tries, plot_eval_over_time\n",
    "\n",
    "warnings.simplefilter('once', UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676f0d10-393a-4b08-90c6-47d4683a4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"secrets/openai_key.txt\", \"r\") as file:\n",
    "    openai_api_key = file.read().strip()\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e249e8-4269-4b7a-a7f2-360bb55eefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS = {}\n",
    "RESULTS = []\n",
    "LAG = 12\n",
    "TRAIN_SIZE = 150\n",
    "PREDICTION_SIZE = 29\n",
    "RESULT_IMAGES_PATH = 'visualizations/project'\n",
    "DF_PATH = os.path.join(\"data\",\"sp500_dates.csv\")\n",
    "DS_NAME = 'sp500'\n",
    "os.makedirs(RESULT_IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb577b-4005-439a-9a34-84aad8e1191c",
   "metadata": {},
   "source": [
    "## Define models ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869da049-a527-4af0-8cd0-88a39d4e8e57",
   "metadata": {},
   "source": [
    "1.  **ARIMA** <br>\n",
    "\n",
    "ARIMA (AutoRegressive Integrated Moving Average) is a statistical model used specifically for univariate time series forecasting. It is a traditional model in econometrics and signal processing.\n",
    "\n",
    "An ARIMA model is defined by three parameters:\n",
    "    \n",
    "    * AR (AutoRegressive): dependence on past values\n",
    "    \n",
    "    * I (Integrated): differencing to make the series stationary\n",
    "    \n",
    "    * MA (Moving Average): dependence on past forecast errors\n",
    "\n",
    "ARIMA models assume that future values of a time series are a linear function of past observations and residual errors. The model explicitly encodes temporal relationships using equations that reflect the structure of the data. It is a classical statistical method specifically developed for time-dependent data. <br/>\n",
    "**sources**: Hayes, A. (2024, July 31). [Autoregressive Integrated Moving Average (ARIMA) prediction model](https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp#:~:text=Autoregressive%20integrated%20moving%20average%20(ARIMA)%20models%20predict%20future%20values%20based,to%20forecast%20future%20security%20prices.). Investopedia. <br>\n",
    "\n",
    "2. **GPT-3 and GPT-4** <br>\n",
    "\n",
    "GPT-3 and GPT-4 are Large Language Models (LLMs) based on the Transformer architecture, which is a deep learning model introduced in the field of Natural Language Processing (NLP). These models are trained using self-supervised learning on massive corpora of text data.\n",
    "GPT models operate using autoregressive language modeling. They generate outputs by predicting the next token (word, number, or symbol) based on preceding context. In essence, they do not \"understand\" in the human sense but rely on statistical correlations learned from text data.\n",
    "    \n",
    "hese models do not reason analytically like traditional algorithms. Instead, they infer likely outcomes based on patterns in their training data. This makes them particularly effective in zero-shot or few-shot learning scenarios where minimal or no task-specific data is provided. **While GPT-3 and GPT-4 are not designed for time series, they can be repurposed by encoding time series as sequences of tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9db5f-c42a-4324-93e0-98db4b9cdee5",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"data/visualizations/comparison_arima_gpts.jpg\" alt=\"Comparison table between ARIMA, GPT-3 and GPT-4 models\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8e78-ce8c-42a6-9de3-829118c2646a",
   "metadata": {},
   "source": [
    "### Setting hyper parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69c8443-a55c-42ec-8f34-fa4c86380bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Function adapted from: https://github.com/ngruver/llmtime\n",
    "    Original author(s): Nicholas Gruver et al.\n",
    "\"\"\"\n",
    "gpt4_hypers = dict(\n",
    "    alpha=0.3,\n",
    "    basic=True,\n",
    "    temp=1.0,\n",
    "    top_p=0.8,\n",
    "    settings=SerializerSettings(base=10, prec=3, signed=True, time_sep=', ', bit_sep='', minus_sign='-')\n",
    ")\n",
    "\n",
    "gpt3_hypers = dict(\n",
    "    temp=0.7,\n",
    "    alpha=0.95,\n",
    "    beta=0.3,\n",
    "    basic=False,\n",
    "    settings=SerializerSettings(base=10, prec=3, signed=True, half_bin_correction=True)\n",
    ")\n",
    "\n",
    "\n",
    "promptcast_hypers = dict(\n",
    "    temp=0.7,\n",
    "    settings=SerializerSettings(base=10, prec=0, signed=True, \n",
    "                                time_sep=', ',\n",
    "                                bit_sep='',\n",
    "                                plus_sign='',\n",
    "                                minus_sign='-',\n",
    "                                half_bin_correction=False,\n",
    "                                decimal_point='')\n",
    ")\n",
    "\n",
    "arima_hypers = dict(p=[12,30], d=[1,2], q=[0])\n",
    "\n",
    "model_hypers = {\n",
    "    #'LLMTime GPT-3.5': {'model': 'gpt-3.5-turbo-instruct', **gpt3_hypers},\n",
    "    #'LLMTime GPT-4': {'model': 'gpt-4', **gpt4_hypers},   \n",
    "    'ARIMA': arima_hypers,\n",
    "    \n",
    "}\n",
    "\n",
    "model_predict_fns = {\n",
    "    #'LLMTime GPT-3.5': get_llmtime_predictions_data,\n",
    "    #'LLMTime GPT-4': get_llmtime_predictions_data,\n",
    "    'ARIMA': get_arima_predictions_data,\n",
    "}\n",
    "\n",
    "model_names = list(model_predict_fns.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599c8a4-f87a-4e4a-ab90-ede2d8db5465",
   "metadata": {},
   "source": [
    "## Dataset Overview ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4d1af3-21ae-4fcf-8d18-b07417f16ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_dataset, split_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3545d94b-0afe-48a0-bc94-d29a1e5e5e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-02</th>\n",
       "      <td>1320.280029</td>\n",
       "      <td>1320.280029</td>\n",
       "      <td>1276.050049</td>\n",
       "      <td>1283.270020</td>\n",
       "      <td>1.129400e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-03</th>\n",
       "      <td>1283.270020</td>\n",
       "      <td>1347.760010</td>\n",
       "      <td>1274.619995</td>\n",
       "      <td>1347.560059</td>\n",
       "      <td>1.880700e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-04</th>\n",
       "      <td>1347.560059</td>\n",
       "      <td>1350.239990</td>\n",
       "      <td>1329.140015</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>2.131000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-05</th>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1334.770020</td>\n",
       "      <td>1294.949951</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1.430800e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-06</th>\n",
       "      <td>1321.676636</td>\n",
       "      <td>1322.630005</td>\n",
       "      <td>1288.729980</td>\n",
       "      <td>1297.519979</td>\n",
       "      <td>1.325700e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-26</th>\n",
       "      <td>869.510010</td>\n",
       "      <td>873.739990</td>\n",
       "      <td>866.520020</td>\n",
       "      <td>872.799988</td>\n",
       "      <td>1.880050e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-27</th>\n",
       "      <td>870.463338</td>\n",
       "      <td>873.726664</td>\n",
       "      <td>863.370015</td>\n",
       "      <td>871.673319</td>\n",
       "      <td>2.361177e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-28</th>\n",
       "      <td>871.416667</td>\n",
       "      <td>873.713338</td>\n",
       "      <td>860.220011</td>\n",
       "      <td>870.546651</td>\n",
       "      <td>2.842303e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-29</th>\n",
       "      <td>872.369995</td>\n",
       "      <td>873.700012</td>\n",
       "      <td>857.070007</td>\n",
       "      <td>869.419983</td>\n",
       "      <td>3.323430e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-30</th>\n",
       "      <td>870.580017</td>\n",
       "      <td>891.119995</td>\n",
       "      <td>870.580017</td>\n",
       "      <td>890.640015</td>\n",
       "      <td>3.627800e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2920 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close        Volume\n",
       "Date                                                                        \n",
       "2001-01-02  1320.280029  1320.280029  1276.050049  1283.270020  1.129400e+09\n",
       "2001-01-03  1283.270020  1347.760010  1274.619995  1347.560059  1.880700e+09\n",
       "2001-01-04  1347.560059  1350.239990  1329.140015  1333.339966  2.131000e+09\n",
       "2001-01-05  1333.339966  1334.770020  1294.949951  1298.349976  1.430800e+09\n",
       "2001-01-06  1321.676636  1322.630005  1288.729980  1297.519979  1.325700e+09\n",
       "...                 ...          ...          ...          ...           ...\n",
       "2008-12-26   869.510010   873.739990   866.520020   872.799988  1.880050e+09\n",
       "2008-12-27   870.463338   873.726664   863.370015   871.673319  2.361177e+09\n",
       "2008-12-28   871.416667   873.713338   860.220011   870.546651  2.842303e+09\n",
       "2008-12-29   872.369995   873.700012   857.070007   869.419983  3.323430e+09\n",
       "2008-12-30   870.580017   891.119995   870.580017   890.640015  3.627800e+09\n",
       "\n",
       "[2920 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sp500_data = df = load_dataset(DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0c6ab3-60a6-43a5-8d33-e3395c4e8208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits from dataset: 16\n",
      "Train:\n",
      "                   Open         High          Low        Close        Volume\n",
      "Date                                                                        \n",
      "2001-05-27  1285.530029  1285.795044  1270.915039  1272.910034  9.270500e+08\n",
      "2001-05-28  1281.710022  1282.107544  1268.162537  1270.420044  9.765250e+08\n",
      "2001-05-29  1277.890015  1278.420044  1265.410034  1267.930054  1.026000e+09\n",
      "2001-05-30  1267.930054  1267.930054  1245.959961  1248.079956  1.158600e+09\n",
      "2001-05-31  1248.079956  1261.910034  1248.069946  1255.819946  1.226600e+09 \n",
      "\n",
      "Test:\n",
      "                   Open         High          Low        Close        Volume\n",
      "Date                                                                        \n",
      "2001-06-01  1255.819946  1265.339966  1246.880005  1260.670044  1.015000e+09\n",
      "2001-06-02  1257.436646  1265.949992  1250.039998  1262.816691  9.555000e+08\n",
      "2001-06-03  1259.053345  1266.560018  1253.199992  1264.963338  8.960000e+08\n",
      "2001-06-04  1260.670044  1267.170044  1256.359985  1267.109985  8.365000e+08\n",
      "2001-06-05  1267.109985  1286.619995  1267.109985  1283.569946  1.116800e+09\n"
     ]
    }
   ],
   "source": [
    "splits = split_time_series(sp500_data, TRAIN_SIZE, PREDICTION_SIZE)\n",
    "\n",
    "print(f'Number of splits from dataset: {len(splits)}')\n",
    "print(\"Train:\")\n",
    "print(splits[0][0].tail(), \"\\n\")\n",
    "print(\"Test:\")\n",
    "print(splits[0][1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc016a9-8838-487c-9cdd-86c41aa73375",
   "metadata": {},
   "source": [
    "## Run Inference ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83280d9-caa3-4c67-8048-83990c5c5b6b",
   "metadata": {},
   "source": [
    "### Metrics. Utility functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c08f02e-a57f-4d08-bfaf-9f21630f549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e608c33-1c27-431c-9437-2f5303769cb1",
   "metadata": {},
   "source": [
    "MAPE measures the average absolute percentage difference between predicted and actual values.\n",
    "\n",
    "![image.png](attachment:7a37d6aa-6a9d-45da-a0cd-6c12e3f6a8eb.png)  TODO: fix image <br/>\n",
    "\n",
    "Interpretation:\n",
    "* Expressed as a percentage, so it shows the average error as a percent of the true values.\n",
    "* Lower MAPE means more accurate predictions relative to the actual values.\n",
    "\n",
    "Scale-independent, so you can compare errors across datasets or variables and intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81356198-b5e7-4456-bf9b-1cda78184ea7",
   "metadata": {},
   "source": [
    "_Simulated Trading Protocol_: In the gain function, we leverage both the predicted open and predicted close prices generated by our models. By comparing these predicted values, we determine whether the model expects the price to increase or decrease within the trading period. We then use these predictions alongside the actual open and close prices to calculate the realized gain, reflecting how accurate the combined open-close forecasts are in predicting profitable movements.\n",
    "\n",
    "* `gain` estimates how much money you would make or lose if you followed the prediction signals (buy if predicted growth, sell if predicted decline) based on actual price movements.\n",
    "* `roi` function normalizes this gain by the average amount invested, giving a percentage-like return figure.\n",
    "* `print_eval` function wraps both, printing out the results clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ede53-d8be-418a-887c-9252b5222b81",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fa676-3bc6-4ef4-97ac-b61c580bd822",
   "metadata": {},
   "source": [
    "#### Get the dataset ready ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdbe8d35-a9a4-4af7-8247-c611b3a32cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.traditional_ml_models import create_lag_features, split_lag_dataset_to_label_and_features, predict_linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1faaf51-dd79-4284-b1ed-4bcdd0cfb171",
   "metadata": {},
   "source": [
    "These functions are used to simulate autoregressive behavior by creating lagged input features that represent past values of the time series. By feeding these lagged features into a linear regression model, we imitate an autoregressive process where predictions depend explicitly on previous observed or predicted values.\n",
    "\n",
    "* `create_lag_features` Create a DataFrame of lagged features from a time series\n",
    "* `split_lag_dataset_to_label_and_features` From multiple (train, test) splits of lagged datasets, separate feature columns and target labels\n",
    "* `update_lag_input` Create a new input row with updated lag features for recursive forecasting\n",
    "* `recursive_forecast` Predict multiple future time points one step at a time, feeding predictions back as inputs, imitating autoregressiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6085428d-baa4-4425-ade8-4371601e2221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_8</th>\n",
       "      <th>lag_9</th>\n",
       "      <th>lag_10</th>\n",
       "      <th>lag_11</th>\n",
       "      <th>lag_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-14</th>\n",
       "      <td>1322.569946</td>\n",
       "      <td>1324.694946</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1310.013306</td>\n",
       "      <td>1321.676636</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1347.560059</td>\n",
       "      <td>1283.270020</td>\n",
       "      <td>1320.280029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-15</th>\n",
       "      <td>1320.444946</td>\n",
       "      <td>1322.569946</td>\n",
       "      <td>1324.694946</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1310.013306</td>\n",
       "      <td>1321.676636</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1347.560059</td>\n",
       "      <td>1283.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-16</th>\n",
       "      <td>1318.319946</td>\n",
       "      <td>1320.444946</td>\n",
       "      <td>1322.569946</td>\n",
       "      <td>1324.694946</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1310.013306</td>\n",
       "      <td>1321.676636</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1347.560059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-17</th>\n",
       "      <td>1326.650024</td>\n",
       "      <td>1318.319946</td>\n",
       "      <td>1320.444946</td>\n",
       "      <td>1322.569946</td>\n",
       "      <td>1324.694946</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1310.013306</td>\n",
       "      <td>1321.676636</td>\n",
       "      <td>1333.339966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-18</th>\n",
       "      <td>1329.890015</td>\n",
       "      <td>1326.650024</td>\n",
       "      <td>1318.319946</td>\n",
       "      <td>1320.444946</td>\n",
       "      <td>1322.569946</td>\n",
       "      <td>1324.694946</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1310.013306</td>\n",
       "      <td>1321.676636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  value        lag_1        lag_2        lag_3        lag_4  \\\n",
       "Date                                                                          \n",
       "2001-01-14  1322.569946  1324.694946  1326.819946  1313.270020  1300.800049   \n",
       "2001-01-15  1320.444946  1322.569946  1324.694946  1326.819946  1313.270020   \n",
       "2001-01-16  1318.319946  1320.444946  1322.569946  1324.694946  1326.819946   \n",
       "2001-01-17  1326.650024  1318.319946  1320.444946  1322.569946  1324.694946   \n",
       "2001-01-18  1329.890015  1326.650024  1318.319946  1320.444946  1322.569946   \n",
       "\n",
       "                  lag_5        lag_6        lag_7        lag_8        lag_9  \\\n",
       "Date                                                                          \n",
       "2001-01-14  1295.859985  1298.349976  1310.013306  1321.676636  1333.339966   \n",
       "2001-01-15  1300.800049  1295.859985  1298.349976  1310.013306  1321.676636   \n",
       "2001-01-16  1313.270020  1300.800049  1295.859985  1298.349976  1310.013306   \n",
       "2001-01-17  1326.819946  1313.270020  1300.800049  1295.859985  1298.349976   \n",
       "2001-01-18  1324.694946  1326.819946  1313.270020  1300.800049  1295.859985   \n",
       "\n",
       "                 lag_10       lag_11       lag_12  \n",
       "Date                                               \n",
       "2001-01-14  1347.560059  1283.270020  1320.280029  \n",
       "2001-01-15  1333.339966  1347.560059  1283.270020  \n",
       "2001-01-16  1321.676636  1333.339966  1347.560059  \n",
       "2001-01-17  1310.013306  1321.676636  1333.339966  \n",
       "2001-01-18  1298.349976  1310.013306  1321.676636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>lag_6</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_8</th>\n",
       "      <th>lag_9</th>\n",
       "      <th>lag_10</th>\n",
       "      <th>lag_11</th>\n",
       "      <th>lag_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-14</th>\n",
       "      <td>1322.600037</td>\n",
       "      <td>1320.575043</td>\n",
       "      <td>1318.550049</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1296.689982</td>\n",
       "      <td>1297.519979</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1347.560059</td>\n",
       "      <td>1283.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-15</th>\n",
       "      <td>1324.625031</td>\n",
       "      <td>1322.600037</td>\n",
       "      <td>1320.575043</td>\n",
       "      <td>1318.550049</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1296.689982</td>\n",
       "      <td>1297.519979</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1333.339966</td>\n",
       "      <td>1347.560059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-16</th>\n",
       "      <td>1326.650024</td>\n",
       "      <td>1324.625031</td>\n",
       "      <td>1322.600037</td>\n",
       "      <td>1320.575043</td>\n",
       "      <td>1318.550049</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1296.689982</td>\n",
       "      <td>1297.519979</td>\n",
       "      <td>1298.349976</td>\n",
       "      <td>1333.339966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-17</th>\n",
       "      <td>1329.469971</td>\n",
       "      <td>1326.650024</td>\n",
       "      <td>1324.625031</td>\n",
       "      <td>1322.600037</td>\n",
       "      <td>1320.575043</td>\n",
       "      <td>1318.550049</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1296.689982</td>\n",
       "      <td>1297.519979</td>\n",
       "      <td>1298.349976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-18</th>\n",
       "      <td>1347.969971</td>\n",
       "      <td>1329.469971</td>\n",
       "      <td>1326.650024</td>\n",
       "      <td>1324.625031</td>\n",
       "      <td>1322.600037</td>\n",
       "      <td>1320.575043</td>\n",
       "      <td>1318.550049</td>\n",
       "      <td>1326.819946</td>\n",
       "      <td>1313.270020</td>\n",
       "      <td>1300.800049</td>\n",
       "      <td>1295.859985</td>\n",
       "      <td>1296.689982</td>\n",
       "      <td>1297.519979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  value        lag_1        lag_2        lag_3        lag_4  \\\n",
       "Date                                                                          \n",
       "2001-01-14  1322.600037  1320.575043  1318.550049  1326.819946  1313.270020   \n",
       "2001-01-15  1324.625031  1322.600037  1320.575043  1318.550049  1326.819946   \n",
       "2001-01-16  1326.650024  1324.625031  1322.600037  1320.575043  1318.550049   \n",
       "2001-01-17  1329.469971  1326.650024  1324.625031  1322.600037  1320.575043   \n",
       "2001-01-18  1347.969971  1329.469971  1326.650024  1324.625031  1322.600037   \n",
       "\n",
       "                  lag_5        lag_6        lag_7        lag_8        lag_9  \\\n",
       "Date                                                                          \n",
       "2001-01-14  1300.800049  1295.859985  1296.689982  1297.519979  1298.349976   \n",
       "2001-01-15  1313.270020  1300.800049  1295.859985  1296.689982  1297.519979   \n",
       "2001-01-16  1326.819946  1313.270020  1300.800049  1295.859985  1296.689982   \n",
       "2001-01-17  1318.550049  1326.819946  1313.270020  1300.800049  1295.859985   \n",
       "2001-01-18  1320.575043  1318.550049  1326.819946  1313.270020  1300.800049   \n",
       "\n",
       "                 lag_10       lag_11       lag_12  \n",
       "Date                                               \n",
       "2001-01-14  1333.339966  1347.560059  1283.270020  \n",
       "2001-01-15  1298.349976  1333.339966  1347.560059  \n",
       "2001-01-16  1297.519979  1298.349976  1333.339966  \n",
       "2001-01-17  1296.689982  1297.519979  1298.349976  \n",
       "2001-01-18  1295.859985  1296.689982  1297.519979  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lag_data_open = create_lag_features(sp500_data['Open'], lag_size=LAG)\n",
    "display(lag_data_open.head())\n",
    "lag_data_close = create_lag_features(sp500_data['Close'], lag_size=LAG)\n",
    "display(lag_data_close.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86768a7d-4bca-403b-a468-ed4a65858ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits from dataset: 16\n",
      "Train:\n",
      "                  value        lag_1        lag_2        lag_3        lag_4  \\\n",
      "Date                                                                          \n",
      "2001-06-08  1276.959961  1270.030029  1283.569946  1267.109985  1260.670044   \n",
      "2001-06-09  1272.959961  1276.959961  1270.030029  1283.569946  1267.109985   \n",
      "2001-06-10  1268.959961  1272.959961  1276.959961  1270.030029  1283.569946   \n",
      "2001-06-11  1264.959961  1268.959961  1272.959961  1276.959961  1270.030029   \n",
      "2001-06-12  1254.390015  1264.959961  1268.959961  1272.959961  1276.959961   \n",
      "\n",
      "                  lag_5        lag_6        lag_7        lag_8        lag_9  \\\n",
      "Date                                                                          \n",
      "2001-06-08  1259.053345  1257.436646  1255.819946  1248.079956  1267.930054   \n",
      "2001-06-09  1260.670044  1259.053345  1257.436646  1255.819946  1248.079956   \n",
      "2001-06-10  1267.109985  1260.670044  1259.053345  1257.436646  1255.819946   \n",
      "2001-06-11  1283.569946  1267.109985  1260.670044  1259.053345  1257.436646   \n",
      "2001-06-12  1270.030029  1283.569946  1267.109985  1260.670044  1259.053345   \n",
      "\n",
      "                 lag_10       lag_11       lag_12  \n",
      "Date                                               \n",
      "2001-06-08  1277.890015  1281.710022  1285.530029  \n",
      "2001-06-09  1267.930054  1277.890015  1281.710022  \n",
      "2001-06-10  1248.079956  1267.930054  1277.890015  \n",
      "2001-06-11  1255.819946  1248.079956  1267.930054  \n",
      "2001-06-12  1257.436646  1255.819946  1248.079956   \n",
      "\n",
      "Test:\n",
      "                  value        lag_1        lag_2        lag_3        lag_4  \\\n",
      "Date                                                                          \n",
      "2001-06-13  1255.849976  1254.390015  1264.959961  1268.959961  1272.959961   \n",
      "2001-06-14  1241.599976  1255.849976  1254.390015  1264.959961  1268.959961   \n",
      "2001-06-15  1219.869995  1241.599976  1255.849976  1254.390015  1264.959961   \n",
      "2001-06-16  1218.033325  1219.869995  1241.599976  1255.849976  1254.390015   \n",
      "2001-06-17  1216.196655  1218.033325  1219.869995  1241.599976  1255.849976   \n",
      "\n",
      "                  lag_5        lag_6        lag_7        lag_8        lag_9  \\\n",
      "Date                                                                          \n",
      "2001-06-13  1276.959961  1270.030029  1283.569946  1267.109985  1260.670044   \n",
      "2001-06-14  1272.959961  1276.959961  1270.030029  1283.569946  1267.109985   \n",
      "2001-06-15  1268.959961  1272.959961  1276.959961  1270.030029  1283.569946   \n",
      "2001-06-16  1264.959961  1268.959961  1272.959961  1276.959961  1270.030029   \n",
      "2001-06-17  1254.390015  1264.959961  1268.959961  1272.959961  1276.959961   \n",
      "\n",
      "                 lag_10       lag_11       lag_12  \n",
      "Date                                               \n",
      "2001-06-13  1259.053345  1257.436646  1255.819946  \n",
      "2001-06-14  1260.670044  1259.053345  1257.436646  \n",
      "2001-06-15  1267.109985  1260.670044  1259.053345  \n",
      "2001-06-16  1283.569946  1267.109985  1260.670044  \n",
      "2001-06-17  1270.030029  1283.569946  1267.109985  \n"
     ]
    }
   ],
   "source": [
    "splits_lagged_open = split_time_series(lag_data_open, TRAIN_SIZE, PREDICTION_SIZE)\n",
    "splits_lagged_close = split_time_series(lag_data_close, TRAIN_SIZE, PREDICTION_SIZE)\n",
    "\n",
    "print(f'Number of splits from dataset: {len(splits_lagged_open)}')\n",
    "print(\"Train:\")\n",
    "print(splits_lagged_open[0][0].tail(), \"\\n\")\n",
    "print(\"Test:\")\n",
    "print(splits_lagged_open[0][1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93354344-0d40-42c5-afb0-b9cb18080191",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_open_train, ys_open_train, Xs_open_test, ys_open_test = split_lag_dataset_to_label_and_features(splits_lagged_open)\n",
    "Xs_close_train, ys_close_train, Xs_close_test, ys_close_test = split_lag_dataset_to_label_and_features(splits_lagged_close)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcadb5-6cc3-420b-85ac-e3e83a784e7c",
   "metadata": {},
   "source": [
    "#### Linear regression ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a52b4d-9e89-4970-bb12-b614c7858998",
   "metadata": {},
   "source": [
    "**Pipeline**: \n",
    "* make predictions with an autoregressive approach for all splits from the original dataset\n",
    "* visualize an Open and a Close series graph of True and Predicted values (chosen randomly)\n",
    "* visualize the histogram of Gain over all the splits, print minimum and maximum values of Gain\n",
    "* visualize the evaluation over time for the split with minimum and maximum Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4da3f2c-8f17-49a0-85eb-5444d8503b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions_open = []\n",
    "for i in range(len(Xs_open_train)):\n",
    "    lr_predictions_open.append(predict_linear_regression((Xs_open_train[i],ys_open_train[i]), (Xs_open_test[i],ys_open_test[i]),lag=LAG))\n",
    "\n",
    "lr_predictions_close = []\n",
    "for i in range(len(Xs_close_train)):\n",
    "    lr_predictions_close.append(predict_linear_regression((Xs_close_train[i],ys_close_train[i]), (Xs_close_test[i],ys_close_test[i]),lag=LAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a18fba1-02e5-4a68-8169-55160f307d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to space limitation the plot is being saved in visualizations/project under the name plot_69087485.png. For running and visualizing it directly in the notebook uncomment and run the commented line\n"
     ]
    }
   ],
   "source": [
    "file_number = random_8_digit_number()\n",
    "#file_number = None\n",
    "print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "plot_autoregressive_ml_model_results(ys_open_train[0], ys_open_test[0], lr_predictions_open[0], ds_name=\"SP500-Open\", model_name='Linear Regression', hash_number=file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3bd15a6-228d-4773-b515-71fbfe4b61e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to space limitation the plot is being saved in visualizations/project under the name plot_85331481.png. For running and visualizing it directly in the notebook uncomment and run the commented line\n"
     ]
    }
   ],
   "source": [
    "file_number = random_8_digit_number()\n",
    "#file_number = None\n",
    "print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "plot_autoregressive_ml_model_results(ys_close_train[2], ys_close_test[2], lr_predictions_close[2], ds_name=\"SP500-Close\", model_name='Linear Regression', hash_number=file_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5d21e-175a-48f6-aa00-7de10bfa2ab9",
   "metadata": {},
   "source": [
    "### Evalutaion ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf872c73-b2f3-4ee2-aaf9-ea175089ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import print_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63993761-0fd3-4790-961c-620bc39cbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gain = []\n",
    "lr_roi = []\n",
    "lr_mape_open = []\n",
    "lr_mape_close = []\n",
    "\n",
    "for i in range(len(lr_predictions_open)):\n",
    "    gain_lr_, roi_lr_ = print_eval(lr_predictions_open[i], lr_predictions_close[i], ys_open_test[i], ys_close_test[i], verbose=False)\n",
    "    lr_mape_open.append(mape(ys_open_test[i], lr_predictions_open[i]))\n",
    "    lr_mape_close.append(mape(ys_close_test[i], lr_predictions_close[i]))\n",
    "    lr_gain.append(gain_lr_)\n",
    "    lr_roi.append(roi_lr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2235c24-8e36-41b2-9d6d-96dc89bad8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to space limitation the plot is being saved in visualizations/project under the name plot_22823587.png. For running and visualizing it directly in the notebook uncomment and run the commented line\n",
      "Gain: Min: -96.17$, Max: 39.5$\n"
     ]
    }
   ],
   "source": [
    "file_number = random_8_digit_number()\n",
    "#file_number = None\n",
    "print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "gain_over_tries(lr_gain,'Linear Regression', hash_number = file_number)\n",
    "print(f\"Gain: Min: {round(min(lr_gain),2)}$, Max: {round(max(lr_gain),2)}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2da0cc20-94f4-4787-81d7-458e4d3e072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to space limitation the plot is being saved in visualizations/project under the name plot_66032252.png. For running and visualizing it directly in the notebook uncomment and run the commented line\n"
     ]
    }
   ],
   "source": [
    "file_number = random_8_digit_number()\n",
    "#file_number = None\n",
    "print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "daily_gain, cumulative_gain_series = plot_eval_over_time(lr_predictions_open[lr_gain.index(min(lr_gain))], \n",
    "                                                         lr_predictions_close[lr_gain.index(min(lr_gain))], \n",
    "                                                         ys_open_test[lr_gain.index(min(lr_gain))], \n",
    "                                                         ys_close_test[lr_gain.index(min(lr_gain))],\n",
    "                                                        hash_number = file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f89030b-cb4e-4f50-b7bd-dba3de2209d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to space limitation the plot is being saved in visualizations/project under the name plot_78752830.png. For running and visualizing it directly in the notebook uncomment and run the commented line\n"
     ]
    }
   ],
   "source": [
    "file_number = random_8_digit_number()\n",
    "#file_number = None\n",
    "print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "daily_gain, cumulative_gain_series = plot_eval_over_time(lr_predictions_open[lr_gain.index(max(lr_gain))],\n",
    "                                                         lr_predictions_close[lr_gain.index(max(lr_gain))],\n",
    "                                                         ys_open_test[lr_gain.index(max(lr_gain))],\n",
    "                                                         ys_close_test[lr_gain.index(max(lr_gain))],\n",
    "                                                        hash_number = file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c472a3cb-b5fb-496a-b3b0-81e4cf899604",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS.append({\n",
    "    \"Model\": \"Linear Regression\",\n",
    "    \"MAPE - Open\": round(sum(lr_mape_open) / len(lr_mape_open),2),\n",
    "    \"MAPE - Closed\": round(sum(lr_mape_close) / len(lr_mape_close),2),\n",
    "    \"Gain\": round(sum(lr_gain) / len(lr_gain),2),\n",
    "    \"ROI\": round(sum(lr_roi) / len(lr_roi),4)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29740bb8-31ff-453c-bd2b-dea2373bc1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RESULTS_df = pd.DataFrame.from_dict(RESULTS)\n",
    "display(RESULTS_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d77dd5-a6e3-4a9a-b7ac-8c40f2728827",
   "metadata": {},
   "source": [
    "### ARIMA, GPT3, GPT4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "058ba4c0-1d84-4e08-8eb3-79dd7e152273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.llmtime import get_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ad8b1b1-507c-4895-8dec-8d5dc4c58e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing split 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suppress_output:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mcapture_output() \u001b[38;5;28;01mas\u001b[39;00m captured:\n\u001b[0;32m---> 15\u001b[0m         model_name_open, predicted_open_dict, model_name_close, predicted_close_dict \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     model_name_open, predicted_open_dict, model_name_close, predicted_close_dict \u001b[38;5;241m=\u001b[39m run_inference()\n",
      "Cell \u001b[0;32mIn[49], line 9\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_inference\u001b[39m():\n\u001b[0;32m----> 9\u001b[0m     model_name_open, predicted_open_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSP500\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model_name_close, predicted_close_dict \u001b[38;5;241m=\u001b[39m get_inference((split[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m], split[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSP500\u001b[39m\u001b[38;5;124m'\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_name_open, predicted_open_dict, model_name_close, predicted_close_dict\n",
      "File \u001b[0;32m~/TM/LLMinStockPrediction/src/models/llmtime.py:46\u001b[0m, in \u001b[0;36mget_inference\u001b[0;34m(data, ds_name, num_samples, visualize)\u001b[0m\n\u001b[1;32m     44\u001b[0m train, test \u001b[38;5;241m=\u001b[39m data \u001b[38;5;66;03m# or change to your own data\u001b[39;00m\n\u001b[1;32m     45\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_names\u001b[49m: \u001b[38;5;66;03m# GPT-4 takes a about a minute to run\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     model_hypers[model]\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m: ds_name}) \u001b[38;5;66;03m# for promptcast\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     hypers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(grid_iter(model_hypers[model]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_names' is not defined"
     ]
    }
   ],
   "source": [
    "suppress_output = True\n",
    "open_dicts_ = {k: [] for k in model_names}\n",
    "close_dicts_ = {k: [] for k in model_names}\n",
    "\n",
    "for i, split in enumerate(splits[:5]):\n",
    "    print(f'[INFO] Processing split {i}')\n",
    "    \n",
    "    def run_inference():\n",
    "        model_name_open, predicted_open_dict = get_inference((split[0]['Open'], split[1]['Open']), 'SP500', num_samples=30, visualize=False)\n",
    "        model_name_close, predicted_close_dict = get_inference((split[0]['Close'], split[1]['Close']), 'SP500', num_samples=30, visualize=False)\n",
    "        return model_name_open, predicted_open_dict, model_name_close, predicted_close_dict\n",
    "\n",
    "    if suppress_output:\n",
    "        with io.capture_output() as captured:\n",
    "            model_name_open, predicted_open_dict, model_name_close, predicted_close_dict = run_inference()\n",
    "    else:\n",
    "        model_name_open, predicted_open_dict, model_name_close, predicted_close_dict = run_inference()\n",
    "\n",
    "    for k in predicted_open_dict:\n",
    "        open_dicts_[k].append(predicted_open_dict[k])\n",
    "    for k in predicted_close_dict:\n",
    "        close_dicts_[k].append(predicted_close_dict[k])\n",
    "    \n",
    "    print(f'[INFO] Processed split {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "707e3e68-6f43-4343-a26f-f812f3573ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Processing model: LLMTime GPT-3.5...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m         ts_gain[model]\u001b[38;5;241m.\u001b[39mappend(preds_model_gain)\n\u001b[1;32m     32\u001b[0m         ts_roi[model]\u001b[38;5;241m.\u001b[39mappend(preds_model_roi)\n\u001b[1;32m     33\u001b[0m     RESULTS\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPE - Open\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mts_mape_open\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mts_mape_open\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPE - Closed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(ts_mape_close[model]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ts_mape_close[model]),\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(ts_gain[model]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ts_gain[model]),\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROI\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(ts_roi[model]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ts_roi[model]),\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m })\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "ts_gain = {}\n",
    "ts_roi = {}\n",
    "ts_mape_open = {}\n",
    "ts_mape_close = {}\n",
    "ts_predictions_open = {}\n",
    "ts_predictions_close = {}\n",
    "\n",
    "for model in model_names:\n",
    "    print(f'[INFO]Processing model: {model}...')\n",
    "    ts_gain[model] = []\n",
    "    ts_roi[model] = []\n",
    "    ts_mape_open[model] = []\n",
    "    ts_mape_close[model] = []\n",
    "    ts_predictions_open[model] = []\n",
    "    ts_predictions_close[model] = []\n",
    "    for i in range(len(open_dicts_[model])):\n",
    "        print(f'\\t\\t\\tProcessing iteration: {i}...')\n",
    "        preds_model_open, preds_model_close =  process_llmtime_outputs(open_dicts_[model][i]['samples'], \n",
    "                                                                       close_dicts_[model][i]['samples'])\n",
    "        \n",
    "        \n",
    "        preds_model_gain, preds_model_roi = print_eval(preds_model_open, \n",
    "                                                       preds_model_close, \n",
    "                                                       splits[i][1]['Open'], \n",
    "                                                       splits[i][1]['Close'],\n",
    "                                                      verbose=False)\n",
    "        ts_predictions_open[model].append(preds_model_open)\n",
    "        ts_predictions_close[model].append(preds_model_close)\n",
    "        ts_mape_open[model].append(mape(splits[i][1]['Open'], preds_model_open))\n",
    "        ts_mape_close[model].append(mape(splits[i][1]['Close'], preds_model_close))\n",
    "        ts_gain[model].append(preds_model_gain)\n",
    "        ts_roi[model].append(preds_model_roi)\n",
    "    RESULTS.append({\n",
    "    \"Model\": model,\n",
    "    \"MAPE - Open\": round(sum(ts_mape_open[model]) / len(ts_mape_open[model]),2),\n",
    "    \"MAPE - Closed\": round(sum(ts_mape_close[model]) / len(ts_mape_close[model]),2),\n",
    "    \"Gain\": round(sum(ts_gain[model]) / len(ts_gain[model]),2),\n",
    "    \"ROI\": round(sum(ts_roi[model]) / len(ts_roi[model]),2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2f7f91f-0b2e-4482-ba67-7f4401bbfec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RESULTS_df = pd.DataFrame.from_dict(RESULTS)\n",
    "display(RESULTS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bba046d9-319e-49a7-92c6-2361bc998e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LLMTime GPT-3.5, gain over tries graph\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_8_digit_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m ts_gain\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, gain over tries graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     file_number \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_8_digit_number\u001b[49m()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#file_number = None\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to space limitation the plot is being saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULT_IMAGES_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m under the name plot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png. For running and visualizing it directly in the notebook uncomment and run the commented line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_8_digit_number' is not defined"
     ]
    }
   ],
   "source": [
    "for model in ts_gain.keys():\n",
    "\n",
    "    print(f\"Model {model}, gain over tries graph\")\n",
    "    file_number = random_8_digit_number()\n",
    "    #file_number = None\n",
    "    print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "    gain_over_tries(ts_gain[model],model, hash_number = file_number)\n",
    "    print(f\"Gain: Min: {round(min(ts_gain[model]),2)}$, Max: {round(max(ts_gain[model]),2)}$\")\n",
    "    \n",
    "    print(f\"Model {model}, minimum gain graph\")\n",
    "    file_number = random_8_digit_number()\n",
    "    #file_number = None\n",
    "    print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "\n",
    "    _ = plot_eval_over_time(ts_predictions_open[model][ts_gain[model].index(min(ts_gain[model]))], \n",
    "                                                         ts_predictions_close[model][ts_gain[model].index(min(ts_gain[model]))], \n",
    "                                                         splits[ts_gain[model].index(min(ts_gain[model]))][1]['Open'], \n",
    "                                                         splits[ts_gain[model].index(min(ts_gain[model]))][1]['Close'],\n",
    "                                                            hash_number=file_number)\n",
    "    print(f\"Model {model}, maximum gain graph\")\n",
    "    file_number = random_8_digit_number()\n",
    "    #file_number = None\n",
    "    print(f\"Due to space limitation the plot is being saved in {RESULT_IMAGES_PATH} under the name plot_{file_number}.png. For running and visualizing it directly in the notebook uncomment and run the commented line\")\n",
    "\n",
    "    _ = plot_eval_over_time(ts_predictions_open[model][ts_gain[model].index(max(ts_gain[model]))], \n",
    "                                                         ts_predictions_close[model][ts_gain[model].index(max(ts_gain[model]))], \n",
    "                                                         splits[ts_gain[model].index(max(ts_gain[model]))][1]['Open'], \n",
    "                                                         splits[ts_gain[model].index(max(ts_gain[model]))][1]['Close'],\n",
    "                                                           hash_number=file_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
